#!/bin/bash
# Nom du job
#SBATCH -J MON_JOB_MPI
#
# Partition visee
#SBATCH --partition=MPI-short
#
# Nombre de noeuds
#SBATCH --nodes=1
# Nombre de processus MPI par noeud
#SBATCH --ntasks-per-node=1
#SBATCH --mem 10000
#
# Temps de presence du job
#SBATCH --time=5:00:00
#
# Adresse mel de l'utilisateur
#
# Envoi des mails
#SBATCH --mail-type=abort,end
#
#SBATCH -o /home/LS2N/thevenin-s/log/job_mpi-ob_tsp_n_50_-u_0_6_10_2.out

module purge
module load intel/2016.3.210
module load intel/mkl/64/2016.3.210
module load intel/mpi/2016.3.210
module load python/3.7.4
module load intel/mkl/64/2017.4.196
module load gcc/12.1.0


export LD_PRELOAD=/lib64/psm2-compat/libpsm_infinipath.so.1

#
# Faire le lien entre SLURM et Intel MPI
export I_MPI_PMI_LIBRARY=/usr/lib64/libpmi.so


srun python main.py 50 -u 0 --full_patterns -t 7200  -minsl 6 -maxsl 10 -shiftgap 2 > /home/LS2N/thevenin-s/log/output-${SLURM_JOB_ID}.txt 
