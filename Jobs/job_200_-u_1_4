#!/bin/bash
# Nom du job
#SBATCH -J MON_JOB_MPI
#
# Partition visee
#SBATCH --partition=MPI-short
#
# Nombre de noeuds
#SBATCH --nodes=1
# Nombre de processus MPI par noeud
#SBATCH --ntasks-per-node=1
#SBATCH --mem 10000
#
# Temps de presence du job
#SBATCH --time=2:00:00
#
# Adresse mel de l'utilisateur
#
# Envoi des mails
#SBATCH --mail-type=abort,end
#
#SBATCH -o /home/LS2N/thevenin-s/log/job_mpi-./Jobs/job_200_-u_1_4.out
 
module purge
module load intel/2016.3.210
module load intel/mkl/64/2016.3.210
module load intel/mpi/2016.3.210
module load python/3.7.4
module load intel/mkl/64/2017.4.196
module load gcc/12.1.0

 
export LD_PRELOAD=/lib64/psm2-compat/libpsm_infinipath.so.1

#
# Faire le lien entre SLURM et Intel MPI
export I_MPI_PMI_LIBRARY=/usr/lib64/libpmi.so


srun python main.py 200 -u 1 --full_patterns -t 3600  -minsl 6 -maxsl 10 -shiftgap 4 > /home/LS2N/thevenin-s/log/output-${SLURM_JOB_ID}.txt 
